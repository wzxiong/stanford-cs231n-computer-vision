{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "\n",
    "conv -> pool -> conv -> pool -> conv -> avg pool -> fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def complex_conv(X_, filters, kernel_size, strides, padding, initializer, regularizer, dropout_rate, is_training_, name):\n",
    "    # dropout -> conv -> bn -> relu\n",
    "    \n",
    "    # combo layers\n",
    "    with tf.variable_scope(name):\n",
    "        out = X_\n",
    "        out = tf.layers.dropout(out, rate=dropout_rate, training=is_training_, name=\"dropout\")\n",
    "        out = tf.layers.conv2d(out, filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, \n",
    "            activation=None, kernel_initializer=initializer, kernel_regularizer=regularizer, name=\"conv\")\n",
    "        out = tf.layers.batch_normalization(out, axis=-1, training=is_training_, name=\"bn\")\n",
    "        out = tf.nn.relu(out)\n",
    "\n",
    "    return out\n",
    "    \n",
    "def model_op(X_, y_, is_training_, reg_rate, dropout_rate=0.):\n",
    "    \n",
    "    xavier_conv2d = tf.contrib.layers.xavier_initializer_conv2d() # use Xavier initializer\n",
    "    xavier_init = tf.contrib.layers.xavier_initializer() # use Xavier initializer\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(reg_rate)\n",
    "\n",
    "    with tf.variable_scope(\"layer1\"):\n",
    "        out = complex_conv(X_, filters=32, kernel_size=[3,3], strides=(1,1) , padding='valid', \n",
    "                    initializer=xavier_conv2d, regularizer=regularizer, dropout_rate=dropout_rate, \n",
    "                    is_training_=is_training_, name=\"conv1\")\n",
    "        out = tf.layers.max_pooling2d(out, pool_size=[2, 2], strides=2, name=\"pool\")\n",
    "\n",
    "    with tf.variable_scope(\"layer2\"):\n",
    "        out = complex_conv(out, filters=64, kernel_size=[2,2], strides=(1,1) , padding='valid', \n",
    "                    initializer=xavier_conv2d, regularizer=regularizer, dropout_rate=dropout_rate, \n",
    "                    is_training_=is_training_, name=\"conv1\")\n",
    "        out = tf.layers.max_pooling2d(out, pool_size=[2, 2], strides=2, name=\"pool\")\n",
    "\n",
    "    with tf.variable_scope(\"layer3\"):\n",
    "        out = complex_conv(out, filters=128, kernel_size=[2,2], strides=(1,1) , padding='valid', \n",
    "                    initializer=xavier_conv2d, regularizer=regularizer, dropout_rate=dropout_rate, \n",
    "                    is_training_=is_training_, name=\"conv1\")\n",
    "        out = tf.layers.average_pooling2d(out, pool_size=[6, 6], strides=1, name=\"pool\")\n",
    "\n",
    "    with tf.variable_scope(\"dense1\"):\n",
    "        out = tf.layers.dropout(out, rate=dropout_rate, training=is_training_, name=\"dropout\")\n",
    "        out = tf.reshape(out, [-1, 128], name=\"pool_flat\")\n",
    "        y_out = tf.layers.dense(out, units=10, activation=None, \n",
    "                    kernel_initializer=xavier_init, kernel_regularizer=regularizer, name=\"output\")\n",
    "                         \n",
    "    return y_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_op(pred, y_):\n",
    "    onehot_labels = tf.one_hot(y_, depth=10)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=onehot_labels)\n",
    "    mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), y_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return mean_loss, accuracy \n",
    "\n",
    "\n",
    "def train_op(mean_loss, start_learn_rate, decay_steps=1000, decay_rate=0.9):\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learn_rate_ = tf.train.exponential_decay(start_learn_rate, global_step, decay_steps,  decay_rate, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate_)\n",
    "    return optimizer.minimize(mean_loss, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def val_model(session, X_val, y_val, X_, y_, is_training_, mean_loss, accuracy, batch_size=64):\n",
    "    batch_val_loss = []\n",
    "    batch_val_acc = []\n",
    "    for i in range(int(math.ceil(X_val.shape[0]/batch_size))):\n",
    "        # generate indicies for the batch\n",
    "        idx = (i*batch_size)%X_val.shape[0]\n",
    "        feed_dict = {X_: X_val[idx:idx+batch_size],\n",
    "                     y_: y_val[idx:idx+batch_size],\n",
    "                     is_training_: False}\n",
    "        loss, acc =  session.run([mean_loss, accuracy], feed_dict=feed_dict)\n",
    "        batch_val_loss.append(loss)\n",
    "        batch_val_acc.append(acc)\n",
    "    val_loss, val_acc = np.mean(batch_val_loss), np.mean(batch_val_acc)\n",
    "    return val_loss, val_acc\n",
    "        \n",
    "def train_model(X_train, y_train, X_val, y_val, learn_rate, reg_rate,\n",
    "              epochs=1, batch_size=64, verbose=True):\n",
    "    \n",
    "    # clear old variables\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # define our input (e.g. the data that changes every batch)\n",
    "    # The first dim is None, and gets sets automatically based on batch size fed in\n",
    "    X_ = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "    y_ = tf.placeholder(tf.int64, [None])\n",
    "    is_training_ = tf.placeholder(tf.bool)\n",
    "\n",
    "    # define graph\n",
    "    pred = model_op(X_, y_, is_training_, reg_rate)\n",
    "    mean_loss, accuracy = loss_op(pred, y_)\n",
    "    train_step = train_op(mean_loss, learn_rate)\n",
    "\n",
    "    session = tf.Session()\n",
    "    # init variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    # keep track of losses and accuracy\n",
    "    epoch_train_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_val_loss = []\n",
    "    epoch_val_acc = []\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        print('Epoch {}'.format(e+1))\n",
    "        # make sure we iterate over the dataset once\n",
    "        batch_train_loss = []\n",
    "        batch_train_acc = []\n",
    "        for i in range(int(math.ceil(X_train.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%X_train.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X_: X_train[idx,:],\n",
    "                         y_: y_train[idx],\n",
    "                         is_training_: True }\n",
    "            # get batch size\n",
    "            actual_batch_size = y_train[i:i+batch_size].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, acc, _ = session.run([mean_loss, accuracy, train_step], feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            batch_train_loss.append(loss)\n",
    "            batch_train_acc.append(acc)\n",
    "            \n",
    "            # print every now and then\n",
    "            if verbose and (iter_cnt % 100) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt, loss, acc))\n",
    "            iter_cnt += 1\n",
    "\n",
    "        # end of epoch\n",
    "        epoch_train_loss.append(np.mean(batch_train_loss))\n",
    "        epoch_train_acc.append(np.mean(batch_train_acc))\n",
    "        val_loss, val_acc = val_model(session, X_val, y_val, X_, y_, is_training_, mean_loss, accuracy)\n",
    "        epoch_val_loss.append(val_loss)\n",
    "        epoch_val_acc.append(val_acc)\n",
    "        print(\"Epoch {0}: training loss = {1:.3g} and accuracy = {2:.3g}, validation loss = {3:.3g} and accuracy = {4:.3g} \"\\\n",
    "              .format(e+1, np.mean(batch_train_loss), np.mean(batch_train_acc), val_loss, val_acc))\n",
    "        \n",
    "        if verbose:\n",
    "            plt.plot(batch_train_loss)\n",
    "            plt.grid(True)\n",
    "            plt.title('Training and Validation Loss')\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "        \n",
    "    train_loss = np.mean(epoch_train_loss)\n",
    "    train_acc = np.mean(epoch_train_acc)\n",
    "    val_loss = np.mean(epoch_val_loss)\n",
    "    val_acc = np.mean(epoch_val_acc)\n",
    "    print(\"Overall train loss = {0:.3g} and accuracy = {1:.3g}, validation loss = {2:.3g} and accuracy = {3:.3g}\"\\\n",
    "          .format(train_loss, train_acc, val_loss, val_acc))\n",
    "    \n",
    "    plt.plot(epoch_train_acc)\n",
    "    plt.plot(epoch_val_acc)\n",
    "    plt.grid(True)\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('epoch number')\n",
    "    plt.ylabel('epoch accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    return train_loss, train_acc, val_loss, val_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "Epoch 1\n",
      "Epoch 1: training loss = 1.4 and accuracy = 0.499, validation loss = 46.1 and accuracy = 0.137 \n",
      "Epoch 2\n",
      "Epoch 2: training loss = 1.15 and accuracy = 0.595, validation loss = 59.8 and accuracy = 0.128 \n",
      "Epoch 3\n",
      "Epoch 3: training loss = 1.05 and accuracy = 0.632, validation loss = 66.2 and accuracy = 0.119 \n",
      "Epoch 4\n",
      "Epoch 4: training loss = 0.978 and accuracy = 0.658, validation loss = 69.9 and accuracy = 0.124 \n",
      "Epoch 5\n",
      "Epoch 5: training loss = 0.924 and accuracy = 0.678, validation loss = 71.9 and accuracy = 0.134 \n",
      "Epoch 6\n",
      "Epoch 6: training loss = 0.882 and accuracy = 0.694, validation loss = 73 and accuracy = 0.145 \n",
      "Epoch 7\n",
      "Epoch 7: training loss = 0.847 and accuracy = 0.706, validation loss = 75.2 and accuracy = 0.14 \n",
      "Epoch 8\n",
      "Epoch 8: training loss = 0.817 and accuracy = 0.717, validation loss = 75.4 and accuracy = 0.145 \n",
      "Epoch 9\n",
      "Epoch 9: training loss = 0.79 and accuracy = 0.727, validation loss = 76.2 and accuracy = 0.154 \n",
      "Epoch 10\n",
      "Epoch 10: training loss = 0.768 and accuracy = 0.737, validation loss = 77.4 and accuracy = 0.152 \n",
      "Overall train loss = 0.96 and accuracy = 0.664, validation loss = 69.1 and accuracy = 0.138\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9e70052470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96044 0.66425 69.0834 0.137793\n"
     ]
    }
   ],
   "source": [
    "#best_val_acc = 0\n",
    "#for reg_rate in [1e-3, 3e-3, 5e-3, 7e-3, 9e-3]:\n",
    "#    print(\"Reg rate {0:.3g}\".format(reg_rate))\n",
    "train_loss, train_acc, val_loss, val_acc = train_model(X_train, y_train, X_val, y_val, \n",
    "    learn_rate=1e-3, reg_rate=0., epochs=10, batch_size=64, verbose=False)\n",
    "#    if val_acc > best_val_acc:\n",
    "#        best_rate = reg_rate\n",
    "#        best_val_acc = val_acc\n",
    "        \n",
    "print(train_loss, train_acc, val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
